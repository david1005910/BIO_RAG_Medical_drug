"""ë¬¸ì„œ ì²˜ë¦¬ API ì—”ë“œí¬ì¸íŠ¸

íŠ¹ì • ë””ë ‰í† ë¦¬ì˜ ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ê³  ê²€ìƒ‰ ì¸ë±ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤.
"""
import json
import logging
import uuid
from typing import List

from fastapi import APIRouter, Depends, File, HTTPException, UploadFile
from pydantic import BaseModel
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.api.deps import get_db
from app.core.config import settings
from app.models.drug import DocumentVector
from app.services.document_service import get_document_service
from app.services.embedding import get_embedding_service

logger = logging.getLogger(__name__)

router = APIRouter()


class DocumentInfo(BaseModel):
    """ë¬¸ì„œ ì •ë³´"""
    filename: str
    filepath: str
    file_type: str
    page_count: int
    content_length: int


class DocumentListResponse(BaseModel):
    """ë¬¸ì„œ ëª©ë¡ ì‘ë‹µ"""
    documents: List[DocumentInfo]
    total: int
    directory: str


class ParseResponse(BaseModel):
    """íŒŒì‹± ì‘ë‹µ"""
    success: bool
    message: str
    parsed_count: int
    documents: List[DocumentInfo]


class IndexResponse(BaseModel):
    """ì¸ë±ì‹± ì‘ë‹µ"""
    success: bool
    message: str
    indexed_count: int
    chunk_count: int


class DocumentSearchResult(BaseModel):
    """ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼"""
    chunk_id: str
    document_id: str
    content: str
    similarity: float


class SearchResponse(BaseModel):
    """ê²€ìƒ‰ ì‘ë‹µ"""
    results: List[DocumentSearchResult]
    query: str


@router.get("/list", response_model=DocumentListResponse)
async def list_documents():
    """ë¬¸ì„œ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ

    ì§€ì› í˜•ì‹: PDF, DOCX, HTML, TXT, MD
    """
    try:
        doc_service = get_document_service()
        documents = doc_service.list_documents()

        doc_infos = []
        for filepath in documents:
            import os
            from pathlib import Path

            filename = Path(filepath).name
            file_ext = Path(filepath).suffix.lower()

            doc_infos.append(DocumentInfo(
                filename=filename,
                filepath=filepath,
                file_type=file_ext[1:] if file_ext else "unknown",
                page_count=0,  # ì•„ì§ íŒŒì‹± ì „
                content_length=os.path.getsize(filepath)
            ))

        return DocumentListResponse(
            documents=doc_infos,
            total=len(doc_infos),
            directory=settings.DOCUMENTS_DIR
        )

    except Exception as e:
        logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/parse", response_model=ParseResponse)
async def parse_documents():
    """ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ë¬¸ì„œ íŒŒì‹±

    ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì¸ë±ì‹±ì€ ë³„ë„ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    try:
        doc_service = get_document_service()
        parsed_docs = await doc_service.parse_all_documents()

        doc_infos = [
            DocumentInfo(
                filename=doc.filename,
                filepath=doc.filepath,
                file_type=doc.file_type,
                page_count=doc.page_count,
                content_length=len(doc.content)
            )
            for doc in parsed_docs
        ]

        return ParseResponse(
            success=True,
            message=f"{len(parsed_docs)}ê°œ ë¬¸ì„œ íŒŒì‹± ì™„ë£Œ",
            parsed_count=len(parsed_docs),
            documents=doc_infos
        )

    except Exception as e:
        logger.error(f"ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/index", response_model=IndexResponse)
async def index_documents(
    session: AsyncSession = Depends(get_db),
    chunk_size: int = settings.DOC_CHUNK_SIZE,
    overlap: int = settings.DOC_CHUNK_OVERLAP,
):
    """ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ê³  ë²¡í„° ì¸ë±ìŠ¤ì— ì¶”ê°€

    Args:
        chunk_size: ì²­í¬ í¬ê¸° (ë¬¸ìž ìˆ˜)
        overlap: ì²­í¬ ê°„ ì˜¤ë²„ëž©

    ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ìž„ë² ë”©ì„ ìƒì„±í•˜ì—¬
    ë²¡í„° DBì— ì €ìž¥í•©ë‹ˆë‹¤.
    """
    try:
        doc_service = get_document_service()
        embedding_service = get_embedding_service()

        # 1. ë¬¸ì„œ íŒŒì‹±
        logger.info("ðŸ“„ ë¬¸ì„œ íŒŒì‹± ì‹œìž‘...")
        parsed_docs = await doc_service.parse_all_documents()

        if not parsed_docs:
            return IndexResponse(
                success=False,
                message="íŒŒì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤",
                indexed_count=0,
                chunk_count=0
            )

        # 2. ì²­í¬ ë¶„í• 
        all_chunks = []
        for doc in parsed_docs:
            chunks = doc_service.chunk_document(
                doc,
                chunk_size=chunk_size,
                overlap=overlap
            )
            all_chunks.extend(chunks)
            logger.info(f"ðŸ“ {doc.filename}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        if not all_chunks:
            return IndexResponse(
                success=False,
                message="ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤",
                indexed_count=len(parsed_docs),
                chunk_count=0
            )

        # 3. ìž„ë² ë”© ìƒì„±
        logger.info(f"ðŸ§  {len(all_chunks)}ê°œ ì²­í¬ ìž„ë² ë”© ìƒì„± ì¤‘...")
        texts = [chunk["content"] for chunk in all_chunks]
        embeddings = await embedding_service.embed_batch(texts)

        # 4. DocumentVector í…Œì´ë¸”ì— ì €ìž¥
        logger.info("ðŸ’¾ ë¬¸ì„œ ë²¡í„° DBì— ì €ìž¥ ì¤‘...")

        # ê¸°ì¡´ ë¬¸ì„œ ë²¡í„° ì‚­ì œ (ìž¬ì¸ë±ì‹± ì‹œ)
        await session.execute(
            DocumentVector.__table__.delete()
        )

        # ìƒˆ ë²¡í„° ì¶”ê°€
        count = 0
        for chunk, embedding in zip(all_chunks, embeddings):
            doc_vector = DocumentVector(
                id=uuid.uuid4(),
                document_id=chunk["document_id"],
                chunk_id=chunk["chunk_id"],
                embedding=embedding,
                content=chunk["content"],
                chunk_index=chunk["metadata"].get("chunk_index", 0),
                extra_data=json.dumps(chunk["metadata"]),
            )
            session.add(doc_vector)
            count += 1

        await session.commit()
        logger.info(f"âœ… {count}ê°œ ë¬¸ì„œ ì²­í¬ ì¸ë±ì‹± ì™„ë£Œ")

        return IndexResponse(
            success=True,
            message=f"{len(parsed_docs)}ê°œ ë¬¸ì„œ, {count}ê°œ ì²­í¬ ì¸ë±ì‹± ì™„ë£Œ",
            indexed_count=len(parsed_docs),
            chunk_count=count
        )

    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
        await session.rollback()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
):
    """ë¬¸ì„œ íŒŒì¼ ì—…ë¡œë“œ

    ì—…ë¡œë“œëœ íŒŒì¼ì„ ë¬¸ì„œ ë””ë ‰í† ë¦¬ì— ì €ìž¥í•©ë‹ˆë‹¤.
    ì¸ë±ì‹±ì€ ë³„ë„ë¡œ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    try:
        import os
        from pathlib import Path

        # íŒŒì¼ í™•ìž¥ìž ê²€ì¦
        filename = file.filename
        file_ext = Path(filename).suffix.lower()
        supported = {'.pdf', '.docx', '.html', '.htm', '.txt', '.md'}

        if file_ext not in supported:
            raise HTTPException(
                status_code=400,
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}. ì§€ì›: {supported}"
            )

        # íŒŒì¼ ì €ìž¥
        filepath = os.path.join(settings.DOCUMENTS_DIR, filename)

        with open(filepath, 'wb') as f:
            content = await file.read()
            f.write(content)

        logger.info(f"âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {filename} ({len(content)} bytes)")

        return {
            "success": True,
            "message": f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {filename}",
            "filepath": filepath,
            "size": len(content)
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/search", response_model=SearchResponse)
async def search_documents(
    query: str,
    top_k: int = 5,
    session: AsyncSession = Depends(get_db),
):
    """ë¬¸ì„œ ë‚´ ê²€ìƒ‰

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜

    Returns:
        ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ ëª©ë¡
    """
    try:
        embedding_service = get_embedding_service()

        # ì¿¼ë¦¬ ìž„ë² ë”©
        query_embedding = await embedding_service.embed_text(query)

        # DocumentVector í…Œì´ë¸”ì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰

        result = await session.execute(
            select(
                DocumentVector.chunk_id,
                DocumentVector.document_id,
                DocumentVector.content,
                DocumentVector.embedding.cosine_distance(query_embedding).label("distance")
            )
            .order_by("distance")
            .limit(top_k)
        )

        rows = result.all()

        search_results = [
            DocumentSearchResult(
                chunk_id=row.chunk_id,
                document_id=row.document_id,
                content=row.content[:500] if row.content else "",  # ë¯¸ë¦¬ë³´ê¸° 500ìž
                similarity=round(1 - row.distance, 4)  # cosine distance to similarity
            )
            for row in rows
        ]

        logger.info(f"ðŸ” ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ: '{query}' -> {len(search_results)}ê°œ ê²°ê³¼")

        return SearchResponse(
            results=search_results,
            query=query
        )

    except Exception as e:
        logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
