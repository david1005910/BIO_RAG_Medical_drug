"""BM25 ê²€ìƒ‰ ì„œë¹„ìŠ¤ - Sparse Search êµ¬í˜„"""
import logging
import re
from typing import Dict, List, Optional, Tuple
from functools import lru_cache

from rank_bm25 import BM25Okapi
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

logger = logging.getLogger(__name__)


class KoreanTokenizer:
    """í•œêµ­ì–´ í† í¬ë‚˜ì´ì € - í˜•íƒœì†Œ ë¶„ì„ ì—†ì´ ë¬¸ì ê¸°ë°˜ í† í°í™”"""

    def __init__(self):
        # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
        self.stopwords = {
            'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼',
            'ëŠ”', 'ì€', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜',
            'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ì—†ë‹¤', 'ì•Šë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤',
            'ê·¸', 'ì €', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
            'ë°', 'ë“±', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ì¤‘', 'ë‚´', 'ìœ„', 'í›„', 'ì „',
            'ì¢€', 'ë„ˆë¬´', 'ë§¤ìš°', 'ì •ë§', 'ì•„ì£¼', 'ë§ì´', 'ì¡°ê¸ˆ', 'ì•½ê°„',
            'í•´ìš”', 'í•©ë‹ˆë‹¤', 'í•´ì£¼ì„¸ìš”', 'ì£¼ì„¸ìš”', 'ì‹¶ì–´ìš”', 'ê°™ì•„ìš”',
        }

        # ì¦ìƒ ê´€ë ¨ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ ë†’ê²Œ)
        self.symptom_keywords = {
            'ë‘í†µ', 'ì—´', 'ë°œì—´', 'ê¸°ì¹¨', 'ì½§ë¬¼', 'ì¬ì±„ê¸°', 'ì¸í›„í†µ', 'ëª©ì•„í””',
            'ë³µí†µ', 'ì„¤ì‚¬', 'ë³€ë¹„', 'êµ¬í† ', 'ì†Œí™”ë¶ˆëŸ‰', 'ì†ì“°ë¦¼', 'ìœ„í†µ',
            'ê·¼ìœ¡í†µ', 'ê´€ì ˆí†µ', 'ìš”í†µ', 'í—ˆë¦¬', 'ì–´ê¹¨', 'ë¬´ë¦',
            'í”¼ë¡œ', 'ë¬´ê¸°ë ¥', 'ê¶Œíƒœ', 'ì¡¸ìŒ', 'ë¶ˆë©´', 'ë‘ë“œëŸ¬ê¸°',
            'ê°€ë ¤ì›€', 'ë°œì§„', 'ì—¼ì¦', 'í†µì¦', 'ë¶“ê¸°', 'ë¶€ì¢…',
            'ì–´ì§€ëŸ¬ì›€', 'í˜„ê¸°ì¦', 'ë©”ìŠ¤êº¼ì›€', 'êµ¬ì—­ì§ˆ',
            'ê°ê¸°', 'ë…ê°', 'ì•Œë ˆë¥´ê¸°', 'ë¹„ì—¼', 'ì²œì‹',
        }

    def tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„ë¦¬

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸

        Returns:
            í† í° ë¦¬ìŠ¤íŠ¸
        """
        if not text:
            return []

        # ì†Œë¬¸ì ë³€í™˜ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = text.lower()
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)

        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        tokens = text.split()

        # ë¶ˆìš©ì–´ ì œê±° ë° ì§§ì€ í† í° ì œê±°
        tokens = [t for t in tokens if t not in self.stopwords and len(t) > 1]

        # N-gram ìƒì„± (2-gram, 3-gram)
        ngrams = []
        for token in tokens:
            ngrams.append(token)

            # í•œê¸€ì¸ ê²½ìš° N-gram ìƒì„±
            if re.match(r'^[ê°€-í£]+$', token) and len(token) >= 2:
                # 2-gram
                for i in range(len(token) - 1):
                    ngrams.append(token[i:i+2])
                # 3-gram (ê¸´ ë‹¨ì–´ì˜ ê²½ìš°)
                if len(token) >= 3:
                    for i in range(len(token) - 2):
                        ngrams.append(token[i:i+3])

            # ì¦ìƒ í‚¤ì›Œë“œë©´ ê°€ì¤‘ì¹˜ ì¶”ê°€ (ì¤‘ë³µ ì¶”ê°€)
            if token in self.symptom_keywords:
                ngrams.append(token)
                ngrams.append(token)

        return ngrams


class BM25SearchService:
    """BM25 ê¸°ë°˜ Sparse ê²€ìƒ‰ ì„œë¹„ìŠ¤"""

    def __init__(self, session: AsyncSession):
        self.session = session
        self.tokenizer = KoreanTokenizer()
        self.bm25: Optional[BM25Okapi] = None
        self.documents: List[Dict] = []
        self.corpus: List[List[str]] = []
        self._initialized = False

    async def initialize(self) -> None:
        """BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” - ëª¨ë“  ì˜ì•½í’ˆ ë¬¸ì„œ ë¡œë“œ"""
        if self._initialized:
            return

        logger.info("ğŸ”§ BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì¤‘...")

        # ëª¨ë“  ì˜ì•½í’ˆ ë¬¸ì„œ ë¡œë“œ
        query = text("""
            SELECT
                id as drug_id,
                item_name,
                entp_name,
                efficacy,
                use_method,
                caution_info,
                side_effects
            FROM drugs
            WHERE efficacy IS NOT NULL
        """)

        result = await self.session.execute(query)
        rows = result.fetchall()

        self.documents = []
        self.corpus = []

        for row in rows:
            # ë¬¸ì„œ ìƒì„± (ê²€ìƒ‰ ëŒ€ìƒ í…ìŠ¤íŠ¸)
            doc_text = self._create_document_text(
                item_name=row.item_name,
                efficacy=row.efficacy,
                use_method=row.use_method,
                caution_info=row.caution_info,
            )

            # í† í°í™”
            tokens = self.tokenizer.tokenize(doc_text)

            if tokens:
                self.documents.append({
                    "drug_id": row.drug_id,
                    "item_name": row.item_name,
                    "entp_name": row.entp_name,
                    "efficacy": row.efficacy,
                    "use_method": row.use_method,
                    "caution_info": row.caution_info,
                    "side_effects": row.side_effects,
                })
                self.corpus.append(tokens)

        # BM25 ì¸ë±ìŠ¤ ìƒì„±
        if self.corpus:
            self.bm25 = BM25Okapi(self.corpus)
            self._initialized = True
            logger.info(f"âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
        else:
            logger.warning("âš ï¸ BM25 ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: ë¬¸ì„œ ì—†ìŒ")

    def _create_document_text(
        self,
        item_name: str,
        efficacy: Optional[str],
        use_method: Optional[str],
        caution_info: Optional[str],
    ) -> str:
        """ê²€ìƒ‰ìš© ë¬¸ì„œ í…ìŠ¤íŠ¸ ìƒì„±"""
        parts = [item_name or ""]

        if efficacy:
            parts.append(efficacy)
        if use_method:
            parts.append(use_method[:200])  # ìš©ë²•ì€ ì•ë¶€ë¶„ë§Œ
        if caution_info:
            parts.append(caution_info[:200])  # ì£¼ì˜ì‚¬í•­ë„ ì•ë¶€ë¶„ë§Œ

        return " ".join(parts)

    async def search(
        self,
        query: str,
        top_k: int = 10,
    ) -> List[Dict]:
        """BM25 ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜

        Returns:
            BM25 ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸
        if not self._initialized:
            await self.initialize()

        if not self.bm25 or not self.documents:
            logger.warning("BM25 ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # ì¿¼ë¦¬ í† í°í™”
        query_tokens = self.tokenizer.tokenize(query)

        if not query_tokens:
            return []

        # BM25 ì ìˆ˜ ê³„ì‚°
        scores = self.bm25.get_scores(query_tokens)

        # ìƒìœ„ kê°œ ê²°ê³¼ ì¶”ì¶œ
        scored_docs = list(zip(range(len(scores)), scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)

        results = []
        for idx, score in scored_docs[:top_k]:
            if score > 0:  # ì ìˆ˜ê°€ 0ë³´ë‹¤ í° ê²ƒë§Œ
                doc = self.documents[idx].copy()
                doc["bm25_score"] = float(score)
                results.append(doc)

        logger.info(f"ğŸ” BM25 ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ (ì¿¼ë¦¬: {query[:30]}...)")
        return results

    async def refresh_index(self) -> None:
        """ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨"""
        self._initialized = False
        self.bm25 = None
        self.documents = []
        self.corpus = []
        await self.initialize()


class HybridSearchService:
    """Hybrid Search - Dense (Vector) + Sparse (BM25) ê²°í•©

    ì ìˆ˜ ì²´ê³„:
    - Dense Score: 0~1 (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    - Sparse Score: 0~50 ê¸°ì¤€ìœ¼ë¡œ 0~1ë¡œ ì •ê·œí™” (50ìœ¼ë¡œ ë‚˜ëˆ”, ìµœëŒ€ 1)
    - Hybrid Score: dense * 0.7 + sparse * 0.3
    """

    # BM25 ì ìˆ˜ ì •ê·œí™” ê¸°ì¤€ (ìµœëŒ€ ì ìˆ˜)
    # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ raw BM25 ì ìˆ˜ê°€ 32-35 ë²”ìœ„ì´ë¯€ë¡œ 50ìœ¼ë¡œ ì„¤ì •
    BM25_MAX_SCORE = 50.0

    def __init__(
        self,
        session: AsyncSession,
        dense_weight: float = 0.7,
        sparse_weight: float = 0.3,
    ):
        self.session = session
        self.bm25_service = BM25SearchService(session)
        self.dense_weight = dense_weight
        self.sparse_weight = sparse_weight

    async def initialize(self) -> None:
        """BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        await self.bm25_service.initialize()

    def _normalize_bm25_score(self, score: float) -> float:
        """BM25 ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™” (50ì  ê¸°ì¤€)

        Args:
            score: ì›ë³¸ BM25 ì ìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ 0~50 ë²”ìœ„)

        Returns:
            0~1 ë²”ìœ„ë¡œ ì •ê·œí™”ëœ ì ìˆ˜
        """
        normalized = score / self.BM25_MAX_SCORE
        return min(normalized, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ

    async def search(
        self,
        query: str,
        dense_results: List[Dict],
        top_k: int = 5,
    ) -> List[Dict]:
        """Hybrid ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            dense_results: Vector search ê²°ê³¼ (similarity í¬í•¨)
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜

        Returns:
            Hybrid ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # BM25 ê²€ìƒ‰ ìˆ˜í–‰
        bm25_results = await self.bm25_service.search(query, top_k=top_k * 3)

        # ê²°ê³¼ë¥¼ drug_idë¡œ ë§¤í•‘
        dense_map: Dict[str, Dict] = {r["drug_id"]: r for r in dense_results}
        bm25_map: Dict[str, Dict] = {r["drug_id"]: r for r in bm25_results}

        # ëª¨ë“  drug_id ìˆ˜ì§‘
        all_drug_ids = set(dense_map.keys()) | set(bm25_map.keys())

        logger.info(f"ğŸ“Š Hybrid ê°€ì¤‘ì¹˜: Dense={self.dense_weight}, Sparse={self.sparse_weight}")

        # Hybrid ì ìˆ˜ ê³„ì‚°
        hybrid_results = []
        for drug_id in all_drug_ids:
            # Dense score: ì›ë³¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0~1)
            if drug_id in dense_map:
                dense_score = dense_map[drug_id].get("similarity", 0)
            else:
                dense_score = 0

            # Sparse score: BM25 ì ìˆ˜ë¥¼ 30ì  ê¸°ì¤€ìœ¼ë¡œ 0~1 ì •ê·œí™”
            if drug_id in bm25_map:
                raw_bm25 = bm25_map[drug_id].get("bm25_score", 0)
                sparse_score = self._normalize_bm25_score(raw_bm25)
            else:
                sparse_score = 0

            # Hybrid score: sparse * 0.7 + dense * 0.3
            hybrid_score = (
                self.sparse_weight * sparse_score +
                self.dense_weight * dense_score
            )

            # ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (dense ìš°ì„ , ì—†ìœ¼ë©´ bm25)
            doc = dense_map.get(drug_id) or bm25_map.get(drug_id)
            if doc:
                result = doc.copy()
                result["dense_score"] = dense_score
                result["bm25_score"] = sparse_score  # ì •ê·œí™”ëœ ì ìˆ˜ (0~1)
                result["hybrid_score"] = hybrid_score

                # similarityëŠ” ì›ë˜ ê°’ ìœ ì§€ (denseê°€ ìˆìœ¼ë©´ ê·¸ ê°’, ì—†ìœ¼ë©´ hybrid)
                if drug_id in dense_map:
                    result["similarity"] = dense_map[drug_id].get("similarity", 0)
                else:
                    result["similarity"] = hybrid_score

                hybrid_results.append(result)

        # Hybrid ì ìˆ˜ë¡œ ì •ë ¬
        hybrid_results.sort(key=lambda x: x["hybrid_score"], reverse=True)

        logger.info(
            f"ğŸ”€ Hybrid ê²€ìƒ‰ ì™„ë£Œ: Dense={len(dense_results)}, "
            f"BM25={len(bm25_results)}, Merged={len(hybrid_results[:top_k])}"
        )

        return hybrid_results[:top_k]


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ (ì„¸ì…˜ë³„ë¡œ ìƒì„±ë˜ë¯€ë¡œ ì‹¤ì œë¡œëŠ” íŒ©í† ë¦¬ íŒ¨í„´ ì‚¬ìš©)
_bm25_service: Optional[BM25SearchService] = None


def get_bm25_service(session: AsyncSession) -> BM25SearchService:
    """BM25 ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return BM25SearchService(session)


def get_hybrid_service(
    session: AsyncSession,
    dense_weight: float = 0.7,
    sparse_weight: float = 0.3,
) -> HybridSearchService:
    """Hybrid Search ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    Args:
        session: DB ì„¸ì…˜
        dense_weight: Dense(ë²¡í„°) ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.7)
        sparse_weight: Sparse(BM25) ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.3)

    Returns:
        HybridSearchService ì¸ìŠ¤í„´ìŠ¤
    """
    return HybridSearchService(session, dense_weight, sparse_weight)
