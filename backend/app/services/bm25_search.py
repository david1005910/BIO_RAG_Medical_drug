"""BM25 ê²€ìƒ‰ ì„œë¹„ìŠ¤ - Sparse Search êµ¬í˜„"""
import asyncio
import logging
import re
from typing import Dict, List, Optional

from rank_bm25 import BM25Okapi
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

logger = logging.getLogger(__name__)


class KoreanTokenizer:
    """í•œêµ­ì–´ í† í¬ë‚˜ì´ì € - í˜•íƒœì†Œ ë¶„ì„ ì—†ì´ ë¬¸ì ê¸°ë°˜ í† í°í™”"""

    def __init__(self):
        # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
        self.stopwords = {
            'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼',
            'ëŠ”', 'ì€', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜',
            'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ì—†ë‹¤', 'ì•Šë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤',
            'ê·¸', 'ì €', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
            'ë°', 'ë“±', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ì¤‘', 'ë‚´', 'ìœ„', 'í›„', 'ì „',
            'ì¢€', 'ë„ˆë¬´', 'ë§¤ìš°', 'ì •ë§', 'ì•„ì£¼', 'ë§ì´', 'ì¡°ê¸ˆ', 'ì•½ê°„',
            'í•´ìš”', 'í•©ë‹ˆë‹¤', 'í•´ì£¼ì„¸ìš”', 'ì£¼ì„¸ìš”', 'ì‹¶ì–´ìš”', 'ê°™ì•„ìš”',
        }

        # ì¦ìƒ ê´€ë ¨ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ ë†’ê²Œ)
        self.symptom_keywords = {
            'ë‘í†µ', 'ì—´', 'ë°œì—´', 'ê¸°ì¹¨', 'ì½§ë¬¼', 'ì¬ì±„ê¸°', 'ì¸í›„í†µ', 'ëª©ì•„í””',
            'ë³µí†µ', 'ì„¤ì‚¬', 'ë³€ë¹„', 'êµ¬í† ', 'ì†Œí™”ë¶ˆëŸ‰', 'ì†ì“°ë¦¼', 'ìœ„í†µ',
            'ê·¼ìœ¡í†µ', 'ê´€ì ˆí†µ', 'ìš”í†µ', 'í—ˆë¦¬', 'ì–´ê¹¨', 'ë¬´ë¦',
            'í”¼ë¡œ', 'ë¬´ê¸°ë ¥', 'ê¶Œíƒœ', 'ì¡¸ìŒ', 'ë¶ˆë©´', 'ë‘ë“œëŸ¬ê¸°',
            'ê°€ë ¤ì›€', 'ë°œì§„', 'ì—¼ì¦', 'í†µì¦', 'ë¶“ê¸°', 'ë¶€ì¢…',
            'ì–´ì§€ëŸ¬ì›€', 'í˜„ê¸°ì¦', 'ë©”ìŠ¤êº¼ì›€', 'êµ¬ì—­ì§ˆ',
            'ê°ê¸°', 'ë…ê°', 'ì•Œë ˆë¥´ê¸°', 'ë¹„ì—¼', 'ì²œì‹',
        }

        # ì¦ìƒ ë™ì˜ì–´ ë§¤í•‘ (ì¼ìƒì–´ â†’ ì˜í•™ ìš©ì–´)
        # í‚¤: ì¼ìƒ í‘œí˜„, ê°’: ê´€ë ¨ ì˜í•™ ìš©ì–´ ë¦¬ìŠ¤íŠ¸
        self.symptom_synonyms = {
            # ë³µë¶€/ì†Œí™” ê´€ë ¨
            'ë°°ê°€': ['ë³µí†µ', 'ë³µë¶€', 'ìœ„í†µ', 'ì¥', 'ì†Œí™”'],
            'ë°°ì•„íŒŒ': ['ë³µí†µ', 'ë³µë¶€í†µì¦', 'ìœ„í†µ', 'ì¥í†µ'],
            'ë°°ì•„í””': ['ë³µí†µ', 'ë³µë¶€í†µì¦', 'ìœ„í†µ'],
            'ì•„íŒŒìš”': ['í†µì¦', 'ë™í†µ'],
            'ì•„íŒŒ': ['í†µì¦', 'ë™í†µ'],
            'ì•„í””': ['í†µì¦', 'ë™í†µ'],
            'ì†ì´': ['ì†Œí™”', 'ìœ„ì¥', 'ì†ì“°ë¦¼', 'ìœ„'],
            'ì†ì“°ë ¤': ['ì†ì“°ë¦¼', 'ìœ„ì—¼', 'ìœ„ì‚°'],
            'ì²´í–ˆ': ['ì†Œí™”ë¶ˆëŸ‰', 'ì²´ê¸°', 'ìœ„ì¥'],
            'ì²´í•´': ['ì†Œí™”ë¶ˆëŸ‰', 'ì²´ê¸°', 'ìœ„ì¥'],
            'ë”ë¶€ë£©': ['ì†Œí™”ë¶ˆëŸ‰', 'ë³µë¶€íŒ½ë§Œ', 'ê°€ìŠ¤'],
            'ê°€ìŠ¤ì°¨': ['ë³µë¶€íŒ½ë§Œ', 'ê°€ìŠ¤', 'ì¥ë‚´ê°€ìŠ¤'],
            'ë³€ì„': ['ë³€ë¹„', 'ì„¤ì‚¬', 'ë°°ë³€'],
            'í™”ì¥ì‹¤': ['ë³€ë¹„', 'ì„¤ì‚¬', 'ë°°ë³€'],
            # ë‘í†µ/ë¨¸ë¦¬ ê´€ë ¨
            'ë¨¸ë¦¬ê°€': ['ë‘í†µ', 'í¸ë‘í†µ', 'ë‘ë¶€'],
            'ë¨¸ë¦¬ì•„íŒŒ': ['ë‘í†µ', 'í¸ë‘í†µ'],
            'ë¨¸ë¦¬ì•„í””': ['ë‘í†µ', 'í¸ë‘í†µ'],
            'ì§€ëˆ': ['ë‘í†µ', 'í¸ë‘í†µ'],
            'ì§€ëˆê±°ë ¤': ['ë‘í†µ', 'í¸ë‘í†µ'],
            # ì—´/ê°ê¸° ê´€ë ¨
            'ì—´ë‚˜': ['ë°œì—´', 'ê³ ì—´', 'ì—´'],
            'ì—´ì´ë‚˜': ['ë°œì—´', 'ê³ ì—´', 'ì—´'],
            'ìœ¼ìŠ¬ìœ¼ìŠ¬': ['ì˜¤í•œ', 'ë°œì—´', 'ê°ê¸°'],
            'ì¶¥ê³ ': ['ì˜¤í•œ', 'ë°œì—´'],
            'ì½§ë¬¼ë‚˜': ['ì½§ë¬¼', 'ë¹„ì—¼', 'ê°ê¸°'],
            'ì½”ë§‰í˜€': ['ì½”ë§‰í˜', 'ë¹„ì—¼', 'ì¶•ë†ì¦'],
            'ê¸°ì¹¨ë‚˜': ['ê¸°ì¹¨', 'í•´ì†Œ', 'ê°€ë˜'],
            'ëª©ì´': ['ì¸í›„í†µ', 'ëª©í†µì¦', 'ì¸í›„ì—¼'],
            'ëª©ì•„íŒŒ': ['ì¸í›„í†µ', 'ëª©í†µì¦', 'ì¸í›„ì—¼'],
            'ëª©ì•„í””': ['ì¸í›„í†µ', 'ëª©í†µì¦'],
            'ë”°ë”': ['ì¸í›„í†µ', 'ëª©í†µì¦'],
            # ê·¼ê³¨ê²© ê´€ë ¨
            'í—ˆë¦¬ê°€': ['ìš”í†µ', 'í—ˆë¦¬í†µì¦', 'ìš”ì¶”'],
            'í—ˆë¦¬ì•„íŒŒ': ['ìš”í†µ', 'í—ˆë¦¬í†µì¦'],
            'ì–´ê¹¨ê°€': ['ì–´ê¹¨í†µì¦', 'ê²¬í†µ', 'ì–´ê¹¨ê²°ë¦¼'],
            'ì–´ê¹¨ì•„íŒŒ': ['ì–´ê¹¨í†µì¦', 'ê²¬í†µ'],
            'ë¬´ë¦ì´': ['ë¬´ë¦í†µì¦', 'ê´€ì ˆí†µ', 'ìŠ¬í†µ'],
            'ë¬´ë¦ì•„íŒŒ': ['ë¬´ë¦í†µì¦', 'ê´€ì ˆí†µ'],
            'ë‹¤ë¦¬ê°€': ['í•˜ì§€í†µì¦', 'ë‹¤ë¦¬í†µì¦', 'ê·¼ìœ¡í†µ'],
            'íŒ”ì´': ['ìƒì§€í†µì¦', 'íŒ”í†µì¦', 'ê·¼ìœ¡í†µ'],
            # í”¼ë¶€ ê´€ë ¨
            'ê°€ë ¤ì›Œ': ['ê°€ë ¤ì›€', 'ì†Œì–‘ì¦', 'í”¼ë¶€ì—¼'],
            'ê°€ë ¤ì›€': ['ê°€ë ¤ì›€', 'ì†Œì–‘ì¦'],
            'ë‘ë“œëŸ¬ê¸°': ['ë‘ë“œëŸ¬ê¸°', 'ë°œì§„', 'í”¼ë¶€ë°œì§„'],
            'ë¾°ë£¨ì§€': ['ì—¬ë“œë¦„', 'ë°œì§„', 'í”¼ë¶€ì—¼'],
            # ê¸°íƒ€ ì¦ìƒ
            'ì–´ì§€ëŸ¬ì›Œ': ['ì–´ì§€ëŸ¬ì›€', 'í˜„ê¸°ì¦', 'ë‘í›ˆ'],
            'ì–´ì§€ëŸ¬ì›€': ['ì–´ì§€ëŸ¬ì›€', 'í˜„ê¸°ì¦'],
            'ë©”ìŠ¤êº¼ì›Œ': ['ë©”ìŠ¤êº¼ì›€', 'êµ¬ì—­', 'êµ¬í† '],
            'í† í• ê²ƒ': ['êµ¬í† ', 'êµ¬ì—­', 'ë©”ìŠ¤êº¼ì›€'],
            'ì ì´': ['ë¶ˆë©´', 'ìˆ˜ë©´', 'ë¶ˆë©´ì¦'],
            'ëª»ì': ['ë¶ˆë©´', 'ìˆ˜ë©´ì¥ì• ', 'ë¶ˆë©´ì¦'],
            'í”¼ê³¤': ['í”¼ë¡œ', 'ê¶Œíƒœ', 'ë¬´ê¸°ë ¥'],
            'í”¼ê³¤í•´': ['í”¼ë¡œ', 'ê¶Œíƒœ', 'ë¬´ê¸°ë ¥'],
            'ëˆˆì´': ['ì•ˆêµ¬í†µì¦', 'ëˆˆí”¼ë¡œ', 'ì•ˆêµ¬ê±´ì¡°'],
            'ëˆˆì•„íŒŒ': ['ì•ˆêµ¬í†µì¦', 'ëˆˆí”¼ë¡œ'],
        }

    def tokenize(self, text: str, expand_synonyms: bool = True) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„ë¦¬

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            expand_synonyms: ì¦ìƒ ë™ì˜ì–´ í™•ì¥ ì—¬ë¶€ (ì¿¼ë¦¬ì—ë§Œ ì ìš©)

        Returns:
            í† í° ë¦¬ìŠ¤íŠ¸
        """
        if not text:
            return []

        # ì†Œë¬¸ì ë³€í™˜ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = text.lower()
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)

        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        tokens = text.split()

        # ë¶ˆìš©ì–´ ì œê±° ë° ì§§ì€ í† í° ì œê±°
        tokens = [t for t in tokens if t not in self.stopwords and len(t) > 1]

        # N-gram ìƒì„± (2-gram, 3-gram)
        ngrams = []
        for token in tokens:
            ngrams.append(token)

            # ë™ì˜ì–´ í™•ì¥ (ì¿¼ë¦¬ì—ë§Œ ì ìš©)
            if expand_synonyms:
                # í† í°ì´ ë™ì˜ì–´ ì‚¬ì „ì— ìˆìœ¼ë©´ ì˜í•™ ìš©ì–´ ì¶”ê°€
                if token in self.symptom_synonyms:
                    for synonym in self.symptom_synonyms[token]:
                        ngrams.append(synonym)
                        # ì˜í•™ ìš©ì–´ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
                        if synonym in self.symptom_keywords:
                            ngrams.append(synonym)

                # í† í°ì˜ ë¶€ë¶„ ë§¤ì¹­ ì²´í¬ (ì˜ˆ: "ë°°ê°€ì•„íŒŒìš”" â†’ "ë°°ê°€", "ì•„íŒŒ")
                for key, synonyms in self.symptom_synonyms.items():
                    if key in token or token in key:
                        for synonym in synonyms:
                            ngrams.append(synonym)

            # í•œê¸€ì¸ ê²½ìš° N-gram ìƒì„±
            if re.match(r'^[ê°€-í£]+$', token) and len(token) >= 2:
                # 2-gram
                for i in range(len(token) - 1):
                    ngram = token[i:i+2]
                    ngrams.append(ngram)
                    # N-gramë„ ë™ì˜ì–´ í™•ì¥ ì ìš©
                    if expand_synonyms and ngram in self.symptom_synonyms:
                        for synonym in self.symptom_synonyms[ngram]:
                            ngrams.append(synonym)
                # 3-gram (ê¸´ ë‹¨ì–´ì˜ ê²½ìš°)
                if len(token) >= 3:
                    for i in range(len(token) - 2):
                        ngrams.append(token[i:i+3])

            # ì¦ìƒ í‚¤ì›Œë“œë©´ ê°€ì¤‘ì¹˜ ì¶”ê°€ (ì¤‘ë³µ ì¶”ê°€)
            if token in self.symptom_keywords:
                ngrams.append(token)
                ngrams.append(token)

        return ngrams


class BM25IndexCache:
    """BM25 ì¸ë±ìŠ¤ ì „ì—­ ìºì‹œ (ì‹±ê¸€í†¤)"""

    _instance = None
    _initialized = False
    _init_lock = None  # asyncio lock for initialization

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.bm25 = None
            cls._instance.documents = []
            cls._instance.corpus = []
            cls._instance.tokenizer = KoreanTokenizer()
            cls._instance._init_lock = asyncio.Lock()
        return cls._instance

    @property
    def is_initialized(self):
        return self._initialized and self.bm25 is not None

    @property
    def lock(self):
        """ì´ˆê¸°í™” ë½ ë°˜í™˜ (ë™ì‹œ ì´ˆê¸°í™” ë°©ì§€)"""
        if self._init_lock is None:
            self._init_lock = asyncio.Lock()
        return self._init_lock

    def set_data(self, bm25, documents, corpus):
        self.bm25 = bm25
        self.documents = documents
        self.corpus = corpus
        self._initialized = True

    def clear(self):
        self.bm25 = None
        self.documents = []
        self.corpus = []
        self._initialized = False


# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
_bm25_cache = BM25IndexCache()


class BM25SearchService:
    """BM25 ê¸°ë°˜ Sparse ê²€ìƒ‰ ì„œë¹„ìŠ¤"""

    def __init__(self, session: AsyncSession):
        self.session = session
        self.cache = _bm25_cache

    @property
    def tokenizer(self):
        return self.cache.tokenizer

    @property
    def bm25(self):
        return self.cache.bm25

    @property
    def documents(self):
        return self.cache.documents

    @property
    def corpus(self):
        return self.cache.corpus

    @property
    def _initialized(self):
        return self.cache.is_initialized

    async def initialize(self) -> None:
        """BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” - ëª¨ë“  ì˜ì•½í’ˆ ë¬¸ì„œ ë¡œë“œ"""
        if self.cache.is_initialized:
            return

        logger.info("ğŸ”§ BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì¤‘...")

        # ëª¨ë“  ì˜ì•½í’ˆ ë¬¸ì„œ ë¡œë“œ
        query = text("""
            SELECT
                id as drug_id,
                item_name,
                entp_name,
                efficacy,
                use_method,
                caution_info,
                side_effects
            FROM drugs
            WHERE efficacy IS NOT NULL
        """)

        result = await self.session.execute(query)
        rows = result.fetchall()

        documents = []
        corpus = []

        for row in rows:
            # ë¬¸ì„œ ìƒì„± (ê²€ìƒ‰ ëŒ€ìƒ í…ìŠ¤íŠ¸)
            doc_text = self._create_document_text(
                item_name=row.item_name,
                efficacy=row.efficacy,
                use_method=row.use_method,
                caution_info=row.caution_info,
            )

            # í† í°í™” (ë¬¸ì„œëŠ” ë™ì˜ì–´ í™•ì¥ ì—†ì´)
            tokens = self.tokenizer.tokenize(doc_text, expand_synonyms=False)

            if tokens:
                documents.append({
                    "drug_id": row.drug_id,
                    "item_name": row.item_name,
                    "entp_name": row.entp_name,
                    "efficacy": row.efficacy,
                    "use_method": row.use_method,
                    "caution_info": row.caution_info,
                    "side_effects": row.side_effects,
                })
                corpus.append(tokens)

        # BM25 ì¸ë±ìŠ¤ ìƒì„± ë° ìºì‹œì— ì €ì¥
        if corpus:
            bm25 = BM25Okapi(corpus)
            self.cache.set_data(bm25, documents, corpus)
            logger.info(f"âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
        else:
            logger.warning("âš ï¸ BM25 ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: ë¬¸ì„œ ì—†ìŒ")

    def _create_document_text(
        self,
        item_name: str,
        efficacy: Optional[str],
        use_method: Optional[str],
        caution_info: Optional[str],
    ) -> str:
        """ê²€ìƒ‰ìš© ë¬¸ì„œ í…ìŠ¤íŠ¸ ìƒì„±"""
        parts = [item_name or ""]

        if efficacy:
            parts.append(efficacy)
        if use_method:
            parts.append(use_method[:200])  # ìš©ë²•ì€ ì•ë¶€ë¶„ë§Œ
        if caution_info:
            parts.append(caution_info[:200])  # ì£¼ì˜ì‚¬í•­ë„ ì•ë¶€ë¶„ë§Œ

        return " ".join(parts)

    async def search(
        self,
        query: str,
        top_k: int = 10,
    ) -> List[Dict]:
        """BM25 ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜

        Returns:
            BM25 ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸
        if not self._initialized:
            await self.initialize()

        if not self.bm25 or not self.documents:
            logger.warning("BM25 ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # ì¿¼ë¦¬ í† í°í™” (ë™ì˜ì–´ í™•ì¥ ì ìš©)
        query_tokens = self.tokenizer.tokenize(query, expand_synonyms=True)

        if not query_tokens:
            return []

        # í™•ì¥ëœ í† í° ë¡œê¹… (ë””ë²„ê·¸ìš©)
        unique_tokens = list(set(query_tokens))[:10]
        logger.debug(f"ğŸ“ í™•ì¥ëœ ì¿¼ë¦¬ í† í°: {unique_tokens}")

        # BM25 ì ìˆ˜ ê³„ì‚°
        scores = self.bm25.get_scores(query_tokens)

        # ìƒìœ„ kê°œ ê²°ê³¼ ì¶”ì¶œ
        scored_docs = list(zip(range(len(scores)), scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)

        results = []
        for idx, score in scored_docs[:top_k]:
            if score > 0:  # ì ìˆ˜ê°€ 0ë³´ë‹¤ í° ê²ƒë§Œ
                doc = self.documents[idx].copy()
                doc["bm25_score"] = float(score)
                results.append(doc)

        logger.info(f"ğŸ” BM25 ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ (ì¿¼ë¦¬: {query[:30]}...)")
        return results

    async def refresh_index(self) -> None:
        """ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨"""
        self.cache.clear()
        await self.initialize()


class HybridSearchService:
    """Hybrid Search - Dense (Vector) + Sparse (BM25) ê²°í•©

    ì ìˆ˜ ì²´ê³„:
    - Dense Score: 0~1 (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    - Sparse Score: 0~30 ê¸°ì¤€ìœ¼ë¡œ 0~1ë¡œ ì •ê·œí™” (30ìœ¼ë¡œ ë‚˜ëˆ”, ìµœëŒ€ 1)
    - Hybrid Score: dense * 0.7 + sparse * 0.3
    """

    # BM25 ì ìˆ˜ ì •ê·œí™” ê¸°ì¤€ (ìµœëŒ€ ì ìˆ˜)
    BM25_MAX_SCORE = 30.0

    def __init__(
        self,
        session: AsyncSession,
        dense_weight: float = 0.7,
        sparse_weight: float = 0.3,
    ):
        self.session = session
        self.bm25_service = BM25SearchService(session)
        self.dense_weight = dense_weight
        self.sparse_weight = sparse_weight

    async def initialize(self) -> None:
        """BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        await self.bm25_service.initialize()

    def _normalize_bm25_score(self, score: float) -> float:
        """BM25 ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™” (30ì  ê¸°ì¤€)

        Args:
            score: ì›ë³¸ BM25 ì ìˆ˜

        Returns:
            0~1 ë²”ìœ„ë¡œ ì •ê·œí™”ëœ ì ìˆ˜
        """
        normalized = score / self.BM25_MAX_SCORE
        return min(normalized, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ

    async def search(
        self,
        query: str,
        dense_results: List[Dict],
        top_k: int = 5,
    ) -> List[Dict]:
        """Hybrid ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            dense_results: Vector search ê²°ê³¼ (similarity í¬í•¨)
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜

        Returns:
            Hybrid ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # BM25 ê²€ìƒ‰ ìˆ˜í–‰
        bm25_results = await self.bm25_service.search(query, top_k=top_k * 3)

        # ê²°ê³¼ë¥¼ drug_idë¡œ ë§¤í•‘
        dense_map: Dict[str, Dict] = {r["drug_id"]: r for r in dense_results}
        bm25_map: Dict[str, Dict] = {r["drug_id"]: r for r in bm25_results}

        # ëª¨ë“  drug_id ìˆ˜ì§‘
        all_drug_ids = set(dense_map.keys()) | set(bm25_map.keys())

        logger.info(f"ğŸ“Š Hybrid ê°€ì¤‘ì¹˜: Dense={self.dense_weight}, Sparse={self.sparse_weight}")

        # Hybrid ì ìˆ˜ ê³„ì‚°
        hybrid_results = []
        for drug_id in all_drug_ids:
            # Dense score: ì›ë³¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0~1)
            if drug_id in dense_map:
                dense_score = dense_map[drug_id].get("similarity", 0)
            else:
                dense_score = 0

            # Sparse score: BM25 ì ìˆ˜ë¥¼ 30ì  ê¸°ì¤€ìœ¼ë¡œ 0~1 ì •ê·œí™”
            if drug_id in bm25_map:
                raw_bm25 = bm25_map[drug_id].get("bm25_score", 0)
                sparse_score = self._normalize_bm25_score(raw_bm25)
            else:
                sparse_score = 0

            # Hybrid score: sparse * 0.7 + dense * 0.3
            hybrid_score = (
                self.sparse_weight * sparse_score +
                self.dense_weight * dense_score
            )

            # ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (dense ìš°ì„ , ì—†ìœ¼ë©´ bm25)
            doc = dense_map.get(drug_id) or bm25_map.get(drug_id)
            if doc:
                result = doc.copy()
                result["dense_score"] = dense_score
                result["bm25_score"] = sparse_score  # ì •ê·œí™”ëœ ì ìˆ˜ (0~1)
                result["hybrid_score"] = hybrid_score

                # similarityëŠ” ì›ë˜ ê°’ ìœ ì§€ (denseê°€ ìˆìœ¼ë©´ ê·¸ ê°’, ì—†ìœ¼ë©´ hybrid)
                if drug_id in dense_map:
                    result["similarity"] = dense_map[drug_id].get("similarity", 0)
                else:
                    result["similarity"] = hybrid_score

                hybrid_results.append(result)

        # Hybrid ì ìˆ˜ë¡œ ì •ë ¬
        hybrid_results.sort(key=lambda x: x["hybrid_score"], reverse=True)

        logger.info(
            f"ğŸ”€ Hybrid ê²€ìƒ‰ ì™„ë£Œ: Dense={len(dense_results)}, "
            f"BM25={len(bm25_results)}, Merged={len(hybrid_results[:top_k])}"
        )

        return hybrid_results[:top_k]


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ (ì„¸ì…˜ë³„ë¡œ ìƒì„±ë˜ë¯€ë¡œ ì‹¤ì œë¡œëŠ” íŒ©í† ë¦¬ íŒ¨í„´ ì‚¬ìš©)
_bm25_service: Optional[BM25SearchService] = None


def get_bm25_service(session: AsyncSession) -> BM25SearchService:
    """BM25 ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return BM25SearchService(session)


def get_hybrid_service(
    session: AsyncSession,
    dense_weight: float = 0.7,
    sparse_weight: float = 0.3,
) -> HybridSearchService:
    """Hybrid Search ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    Args:
        session: DB ì„¸ì…˜
        dense_weight: Dense(ë²¡í„°) ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.7)
        sparse_weight: Sparse(BM25) ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.3)

    Returns:
        HybridSearchService ì¸ìŠ¤í„´ìŠ¤
    """
    return HybridSearchService(session, dense_weight, sparse_weight)


async def initialize_bm25() -> bool:
    """ì„œë²„ ì‹œì‘ ì‹œ BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ë³„ë„ ì„¸ì…˜ ì‚¬ìš©)

    Returns:
        ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
    """
    if _bm25_cache.is_initialized:
        logger.info("âœ… BM25 ì¸ë±ìŠ¤ ì´ë¯¸ ì´ˆê¸°í™”ë¨")
        return True

    try:
        from app.db.session import async_session_maker

        logger.info("ğŸ”§ BM25 ì¸ë±ìŠ¤ ì‹œì‘ ì‹œ ì´ˆê¸°í™” ì¤‘...")

        async with async_session_maker() as session:
            bm25_service = BM25SearchService(session)
            await bm25_service.initialize()

        return _bm25_cache.is_initialized

    except Exception as e:
        logger.error(f"âŒ BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False
